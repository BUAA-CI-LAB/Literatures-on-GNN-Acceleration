# Papers For Deep Graph Learning Acceleration
Deep graph learning acceleration papers in a chronological order.

[Back to home](./README.md)

---
## Journal



### 2022

* [**TC 2022**] Multi-node Acceleration for Large-scale GCNs. *Sun, Gongjian, et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9893364/)
* [**CAL 2022**] Characterizing and Understanding HGNNs on GPUs. *Yan M, Zou M, Yang X, et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9855397/)
* [**CAL**] Characterizing and Understanding Distributed GNN Training on GPUs. *Lin et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9760056)
* [**JSA**] Algorithms and architecture support of degree-based quantization for graph neural networks. *Guo et al.* [[Paper]](https://dl.acm.org/doi/10.1016/j.sysarc.2022.102578)
* [**JSA**] QEGCN: An FPGA-based accelerator for quantized GCNs with edge-level parallelism. *Yuan et al.* [[Paper]](https://dl.acm.org/doi/10.1016/j.sysarc.2022.102596)
* [**TPDS**] SGCNAX: A Scalable Graph Convolutional Neural Network Accelerator With Workload Balancing. *Li et al.* [[Paper]](https://www.computer.org/csdl/journal/td/2022/11/09645224/1zc6JTLADC0)
* [**TCSI**] A Low-Power Graph Convolutional Network Processor With Sparse Grouping for 3D Point Cloud Semantic Segmentation in Mobile Devices. *Kim et al.* [[Paper]](https://ieeexplore.ieee.org/document/9669025)


### 2021

* [**CAL**] Making a Better Use of Caches for GCN Accelerators with Feature Slicing and Automatic Tile Morphing. [[Paper]](https://ieeexplore.ieee.org/document/9461595/)
* [**CAL**] Hardware Acceleration for GCNs via Bidirectional Fusion. *Li et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9425440)
* [**JPDC**] Accurate, efficient and scalable training of Graph Neural Networks *Zeng et al.* [[Paper]](https://www.sciencedirect.com/science/article/pii/S0743731520303579)
* [**JPDC**] High performance GPU primitives for graph-tensor learning operations. *Zhang et al.* [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0743731520304007)
* [**PMLR**] A Unified Lottery Ticket Hypothesis for Graph Neural Networks. *Chen et al.* [[Paper]](http://proceedings.mlr.press/v139/chen21p.html)
* [**PVLDB**] Accelerating Large Scale Real-Time GNN Inference using Channel Pruning. *Zhou et al.* [[Paper]](https://doi.org/10.14778/3461535.3461547)
* [**SCIS**] Towards Efficient Allocation of Graph Convolutional Networks on Hybrid Computation-In-Memory Architecture. *Chen et al.* [[Paper]](https://link.springer.com/article/10.1007/s11432-020-3248-y)
* [**TCAD**] Cambricon-G: A Polyvalent Energy-efficient Accelerator for Dynamic Graph Neural Networks. *Song et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9326339)
* [**TCAD**] Rubik: A Hierarchical Architecture for Efficient Graph Neural Network Training. *Chen et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9428002)
* [**TPDS**] Efficient Data Loader for Fast Sampling-Based GNN Training on Large Graphs. *Bai et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9376972)

### 2020

* [**CAL**] Characterizing and Understanding GCNs on GPU. *Yan et al.* [[Paper]](https://arxiv.org/abs/2010.00130)
* [**CCIS**] GNN-PIM: A Processing-in-Memory Architecture for Graph Neural Networks. *Wang et al.* [[Paper]](https://www.semanticscholar.org/paper/GNN-PIM%3A-A-Processing-in-Memory-Architecture-for-Wang-Guan/1d03e4bebc9cf3c3fdd9204504d92b20d97d1fdf)
* [**IEEE Access**] FPGAN: An FPGA Accelerator for Graph Attention Networks With Software and Hardware Co-Optimization. *Yan et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9195849)
* [**NeurIPS**] Gcn meets gpu: Decoupling “when to sample” from “how to sample”.*Morteza et al.* [[Paper]](https://proceedings.neurips.cc/paper/2020/file/d714d2c5a796d5814c565d78dd16188d-Paper.pdf)
* [**TC**] EnGN: A High-Throughput and Energy-Efficient Accelerator for Large Graph Neural Networks. *Liang et al.* [[Paper]](https://ieeexplore.ieee.org/document/9161360/)
* [**TKDE**] Deep Learning on Graphs: A Survey. *Zhang et al.*[[paper]](https://ieeexplore.ieee.org/abstract/document/9039675)
* [**TPDS**] EDGES: An Efficient Distributed Graph Embedding System on GPU Clusters. *Yang et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9272876)

### 2019
* [**CoRR**] Deep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs. *Wang* [[Paper]](https://arxiv.org/abs/1909.01315v2) [[GitHub]](https://github.com/dmlc/dgl/) [[Home Page]](https://www.dgl.ai/)


---
## Conference



### 2022
* [**VLDB 2022**] ByteGNN: efficient graph neural network training at large scale. *Zheng C, Chen H, Cheng Y, et al.* [[Paper]](https://dl.acm.org/doi/abs/10.14778/3514061.3514069)
* [**SIGMOD**] NeutronStar: Distributed GNN Training with Hybrid Dependency Management. *Wang et al.* [[Paper]](https://doi.org/10.1145/3514221.3526134)
* [**DAC**] Improving GNN-Based Accelerator Design Automation with Meta Learning. *Bai et al.* [[Paper]](https://vast.cs.ucla.edu/sites/default/files/publications/_DAC_22__GNN_DSE_MAML.pdf)
* [**ISCA**] Hyperscale FPGA-as-a-service architecture for large-scale distributed graph neural network. *Li et al.* [[Paper]](https://dl.acm.org/doi/10.1145/3470496.3527439)
* [**ISCA**] Graphite: optimizing graph neural networks on CPUs through cooperative software-hardware techniques. *Gong et al.* [[Paper]](https://dl.acm.org/doi/10.1145/3470496.3527403)
* [**ISCA**] DIMMining: pruning-efficient and parallel graph mining on near-memory-computing. *Dai et al.* [[Paper]](https://doi.org/10.1145/3470496.3527388)
* [**ICML**] GraphFM: Improving Large-Scale GNN Training via Feature Momentum. *Yu et al.* [[Paper]](https://arxiv.org/abs/2206.07161)
* [**ICML**] Generalization Guarantee of Training Graph Convolutional Networks with Graph Topology Sampling. *Li et al.* [[Paper]](https://icml.cc/Conferences/2022/Schedule?showEvent=16764)
* [**ICML**] Efficient Computation of Higher-Order Subgraph Attribution via Message Passing. *Xiong et al.* [[Paper]](https://icml.cc/Conferences/2022/Schedule?showEvent=17546)
* [**ICML**] Scalable Deep Gaussian Markov Random Fields for General Graphs. *Oskarsson et al.* [[Paper]](https://arxiv.org/abs/2206.05032)
* [**ICML**] LeNSE: Learning To Navigate Subgraph Embeddings for Large-Scale Combinatorial Optimisation. *Ireland et al.* [[Paper]](https://arxiv.org/abs/2205.10106)
* [**CICC**] StreamGCN: Accelerating Graph Convolutional Networks with Streaming Processing. *Sohrabizadeh et al.* [[Paper]](https://web.cs.ucla.edu/~atefehsz/publication/StreamGCN-CICC22.pdf)
* [**ICLR**] EXACT: Scalable Graph Neural Networks Training via Extreme Activation Compression. *Liu et al.* [[Paper]](https://openreview.net/pdf?id=vkaMaq95_rX)
* [**ICLR**] Learn Locally, Correct Globally: A Distributed Algorithm for Training Graph Neural Networks. *Ramezani et al.* [[Paper]](https://openreview.net/forum?id=FndDxSz3LxQ)
* [**ICLR**] Graph-less Neural Networks: Teaching Old MLPs New Tricks Via Distillation. *Zhang et al.* [[Paper]](https://openreview.net/forum?id=4p6_5HBWPCw)
* [**ICLR**] Adaptive Filters for Low-Latency and Memory-Efficient Graph Neural Networks. *Tailor et al.* [[Paper]](https://openreview.net/forum?id=hl9ePdHO4_s)
* [**PPoPP**] QGTC: accelerating quantized graph neural networks via GPU tensor core. *Wang et al.* [[Paper]](https://dl.acm.org/doi/10.1145/3503221.3508408)
* [**WWW**] Resource-Efficient Training for Large Graph Convolutional Networks with Label-Centric Cumulative Sampling. *Lin et al.* [[Paper]](https://dl.acm.org/doi/10.1145/3485447.3512165)
* [**WWW**] PaSca: A Graph Neural Architecture Search System under the Scalable Paradigm. *Zhang et al.* [[Paper]](https://dl.acm.org/doi/10.1145/3485447.3511986)
* [**WWW**] Fograph: Enabling Real-Time Deep Graph Inference with Fog Computing. *Zeng et al.* [[Paper]](https://dl.acm.org/doi/abs/10.1145/3485447.3511982)
* [**MLSys**] Accelerating Training and Inference of Graph Neural Networks with Fast Sampling and Pipelining. *Kaler et al.* [[Paper]](https://proceedings.mlsys.org/paper/2022/hash/35f4a8d465e6e1edc05f3d8ab658c551-Abstract.html)
* [**MLSys**] BNS-GCN: Efficient Full-Graph Training of Graph Convolutional Networks with Boundary Node Sampling. *Wan et al.* [[Paper]](https://arxiv.org/pdf/2203.10983.pdf)
* [**MLSys**] Graphiler: Optimizing Graph Neural Networks with Message Passing Data Flow Graph. *Xie et al.* [[Paper]](https://proceedings.mlsys.org/paper/2022/hash/a87ff679a2f3e71d9181a67b7542122c-Abstract.html)
* [**MLSys**] Sequential Aggregation and Rematerialization: Distributed Full-batch Training of Graph Neural Networks on Large Graphs. *Mostafa.* [[Paper]](https://proceedings.mlsys.org/paper/2022/hash/5fd0b37cd7dbbb00f97ba6ce92bf5add-Abstract.html)
* [**MLSys**] Understanding GNN Computational Graph: A Coordinated Computation, IO, and Memory Perspective. *Zhang et al.* [[Paper]](http://arxiv.org/abs/2110.09524)
* [**ICLR**] PipeGCN: Efficient full-graph training of graph convolutional networks with pipelined feature communication. *Wan et al.* [[Paper]](https://arxiv.org/pdf/2203.10428.pdf)
* [**AAAI**] Early-Bird GCNs: Graph-Network Co-Optimization Towards More Efficient GCN Training and Inference via Drawing Early-Bird Lottery Tickets. *You et al.* [[Paper]](https://www.researchgate.net/profile/Haoran-You/publication/349704520_GEBT_Drawing_Early-Bird_Tickets_in_Graph_Convolutional_Network_Training/links/61e0930dc5e3103375916c9f/GEBT-Drawing-Early-Bird-Tickets-in-Graph-Convolutional-Network-Training.pdf)
* [**HPCA**] GCoD: Graph Convolutional Network Acceleration via Dedicated Algorithm and Accelerator Co-Design. *You et al.* [[Paper]](https://arxiv.org/pdf/2112.11594)
* [**HPCA**] Accelerating Graph Convolutional Networks Using Crossbar-based Processing-In-Memory Architectures. *Huang et al.* [[Paper]](https://www.computer.org/csdl/proceedings-article/hpca/2022/202700b029/1Ds0gRvUFjO)
* [**HPCA**] ReGNN: A Redundancy-Eliminated Graph Neural Networks Accelerator. *Chen et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9773273)
* [**HPDC**] TLPGNN: A Lightweight Two-Level Parallelism Paradigm for Graph Neural Network Computation on GPU. *Fu et al.* [[Paper]](https://doi.org/10.1145/3502181.3531467)
* [**ICLR**] IGLU: Efficient GCN Training via Lazy Updates. *Narayanan et al.* [[Paper]](https://arxiv.org/pdf/2109.13995)
* [**FAST**] Hardware/Software Co-Programmable Framework for Computational SSDs to Accelerate Deep Learning Service on Large-Scale Graphs. *Kwon, et al.* [[Paper]](https://arxiv.org/abs/2201.09189)
* [**FCCM**] GenGNN: A Generic FPGA Framework for Graph Neural Network Acceleration. *Stefan, et al.* [[Paper]]( https://arxiv.org/pdf/2201.08475)
* [**FPGA**] HP-GNN: Generating High Throughput GNN Training Implementation on CPU-FPGA Heterogeneous Platform. *Lin, et al.* [[Paper]]( https://dl.acm.org/doi/pdf/10.1145/3490422.3502359)  
* [**FPGA**] DecGNN: A Framework for Mapping Decoupled GNN Models onto CPU-FPGA Heterogeneous Platform. *Zhang, et al.* [[Paper]]( https://dl.acm.org/doi/abs/10.1145/3490422.3502326)
* [**FPGA**] SPA-GCN: Efficient and Flexible GCN Accelerator with Application for Graph Similarity Computation. *Atefeh, et al.* [[Paper]]( https://arxiv.org/pdf/2111.05936)
* [**EuroSys**] GNNLab: a factored system for sample-based GNN training over GPUs. *Yang J, Tang D, Song X, et al.* [[Paper]](https://dl.acm.org/doi/abs/10.1145/3492321.3519557)

### 2021
* [**KDD**] Global Neighbor Sampling for Mixed CPU-GPU Training on Giant Graphs. *Dong et al.* [[Paper]](https://dl.acm.org/doi/10.1145/3447548.3467437)
* [**KDD**] Performance-Adaptive Sampling Strategy Towards Fast and Accurate Graph Neural Networks. *Yoon et al.* [[Paper]](https://dl.acm.org/doi/10.1145/3447548.3467284)
* [**ICDM**] GraphANGEL: Adaptive aNd Structure-Aware Sampling on Graph NEuraL Networks. *Peng et al.* [[Paper]](https://ieeexplore.ieee.org/document/9679081)
* [**KDD**] DeGNN: Improving Graph Neural Networks with Graph Decomposition. *Miao, et al.* [[Paper]](https://dl.acm.org/doi/10.1145/3447548.3467312)
* [**ICML**] Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth. *Xu et al.* [[Paper]](http://proceedings.mlr.press/v139/xu21k/xu21k.pdf)
* [**ICML**] GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training. *Cai et al.* [[Paper]](http://proceedings.mlr.press/v139/cai21e/cai21e.pdf)
* [**ICCAD**] DARe: DropLayer-Aware Manycore ReRAM architecture for Training Graph Neural Networks. *Aqeeb et al.* [[Paper]](https://ieeexplore.ieee.org/document/9643511)
* [**DAC**] PIMGCN: A ReRAM-Based PIM Design for Graph Convolutional Network Acceleration. *Yang, et al.*  [[Paper]](https://ieeexplore.ieee.org/document/9586231)
* [**NeurIPS**] Graph Differentiable Architecture Search with Structure Learning. *Qin et al.* [[Paper]](https://papers.nips.cc/paper/2021/file/8c9f32e03aeb2e3000825c8c875c4edd-Paper.pdf)
* [**DAC**] DyGNN: Algorithm and Architecture Support of Dynamic Pruning for Graph Neural Networks. *Chen et al.* [[Paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9586298)
* [**CLUSTER**] 2PGraph: Accelerating GNN Training over Large Graphs on GPU Clusters.*Zhang et al.* [[Paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9556026)
* [**MICRO**] Point-X: A Spatial-Locality-Aware Architecture for Energy-Efficient Graph-Based Point-Cloud Deep Learning. *Zhang, et al.*  [[Paper]](https://dl.acm.org/doi/10.1145/3466752.3480081)
* [**ISCAS**] Characterizing the Communication Requirements of GNN Accelerators: A Model-Based Approach. *Guirado et al.* [[Paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9401612)
* [**OSDI**] GNNAdvisor: An Adaptive and Efﬁcient Runtime System for GNN Acceleration on GPUs *Wang et al.* [[Paper]](https://www.usenix.org/system/files/osdi21-wang-yuke.pdf)
* [**GNNSys Workshop**] Adaptive Filters and Aggregator Fusion for Efficient Graph Convolutions.*Tailor et al.* [[Paper]](https://arxiv.org/abs/2104.01481) [[GitHub]](https://github.com/shyam196/egc)
* [**CVPR**] Bi-GCN: Binary Graph Convolutional Network. *Wang et al.* [[Paper]](https://arxiv.org/abs/2010.07565)
* [**CVPR**] Binary Graph Neural Networks. *Bahri et al.* [[Paper]](https://arxiv.org/abs/2012.15823)
* [**DAC**] BlockGNN: Towards Efficient GNN Acceleration Using Block-Circulant Weight Matrices. *Zhou, et al.* [[Paper]](https://arxiv.org/abs/2104.06214)
* [**DAC**] GNNerator: A Hardware/Software Framework for Accelerating Graph Neural Networks. *Stevens et al.* [[Paper]](https://arxiv.org/abs/2103.10836)
* [**DATE**] ReGraphX: NoC-Enabled 3D Heterogeneous ReRAM Architecture for Training Graph Neural Networks. *Arka, et al.* [[Paper]](https://doi.org/10.23919/DATE51398.2021.9473949)
* [**EuroSys**] Tesseract: distributed, general graph pattern mining on evolving graphs. *Bindschaedler, et al.* [[Paper]](https://doi.org/10.1145/3447786.3456253)
* [**EuroSys**] Accelerating Graph Sampling for Graph Machine Learning Using GPUs. *Jangda, et al.* [[Paper]]( https://doi.org/10.1145/3447786.3456244)
* [**EuroSys**] DGCL: an efficient communication library for distributed GNN training. *Cai et al.* [[Paper]](https://dl.acm.org/doi/abs/10.1145/3447786.3456233)
* [**EuroSys**] FlexGraph: A Flexible and Efficient Distributed Framework for GNN Training. *Wang, et al.* [[Paper]](https://doi.org/10.1145/3447786.3456229)
* [**EuroSys**] Seastar: vertex-centric programming for graph neural networks. *Wu, et al.* [[Paper]](https://doi.org/10.1145/3447786.3456247)
* [**FCCM**] BoostGCN: A Framework for Optimizing GCN Inference on FPGA. *Zhang et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9444065)
* [**HPCA**] GCNAX: A Flexible and Energy-Efficient Accelerator for Graph Convolutional Neural Networks.  *Li, et al.*  [[Paper]](https://doi.org/10.1109/HPCA51647.2021.00070)
* [**ICLR**] Degree-Quant: Quantization-Aware Training for Graph Neural Networks. *Tailor et al.* [[Paper]](https://arxiv.org/abs/2008.05000)
* [**ICLR (Open Review)**] FGNAS: FPGA-AWARE GRAPH NEURAL ARCHITECTURE SEARCH. *Qing et al.* [[Paper]](https://openreview.net/pdf?id=cq4FHzAz9eA) 
* [**IPDPS**] FusedMM: A Unified SDDMM-SpMM Kernel for Graph Embedding and Graph Neural Networks. *Rahman et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9460486)
* [**ISPASS**] GNNMark: A Benchmark Suite to Characterize Graph Neural Network Training on GPUs. *Baruah et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9408205)
* [**ISPASS**] Performance Analysis of Graph Neural Network Frameworks. *Wu et al.* [[Paper]](https://www.semanticscholar.org/paper/Performance-Analysis-of-Graph-Neural-Network-Wu-Sun/b6da3ab0a6e710f16e11e5890818a107d1d5735c)
* [**MICRO**] AWB-GCN: A Graph Convolutional Network Accelerator with Runtime Workload Rebalancing. *Geng et al.* [[Paper]](https://ieeexplore.ieee.org/document/9252000)
* [**OSDI**] Dorylus: Affordable, Scalable, and Accurate GNN Training with Distributed CPU Servers and Serverless Threads. *Thorpe et al.* [[Paper]](https://arxiv.org/abs/2105.11118) [[GitHub]](https://github.com/uclasystem/dorylus)
* [**PPoPP**] Understanding and Bridging the Gaps in Current GNN Performance Optimizations. *Huang, et al.* [[Paper]](https://doi.org/10.1145/3437801.3441585)
* [**RTAS**] Brief Industry Paper: optimizing Memory Efficiency of Graph Neural Networks on Edge Computing Platforms. *Zhou et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/820993)
* [**USENIX ATC**] GLIST: Towards In-Storage Graph Learning. *Li, et al.* [[Paper]](www.usenix.org/conference/atc21/presentation/li-cangyuan)
* [**GLSVLSI**] Co-Exploration of Graph Neural Network and Network-on-Chip Design Using AutoML. *Manu et al.* [[Paper]](https://dl.acm.org/doi/abs/10.1145/3453688.3461741)
* [**ICCAD**] G-CoS: GNN-Accelerator Co-Search Towards Both Better Accuracy and Efficiency. *Zhang et al.* [[Paper]](https://arxiv.org/pdf/2109.08983.pdf)
* [**IJCAI**] Automated Machine Learning on Graphs: A Survey. *Zhang et al.* [[Paper]](https://arxiv.org/abs/2103.00742)
* [**GNNSys Workshop**] FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks.*He et al.* [[Paper]](https://gnnsys.github.io/papers/GNNSys21_paper_3.pdf) [[Poster]](https://gnnsys.github.io/posters/GNNSys21_poster_3.pdf)
* [**GNNSys Workshop**] IGNNITION: A framework for fast prototyping of Graph Neural Networks.*Pujol-Perich et al.* [[Paper]](https://gnnsys.github.io/papers/GNNSys21_paper_4.pdf) [[Poster]](https://gnnsys.github.io/posters/GNNSys21_poster_4.pdf)
* [**GNNSys Workshop**] Efficient Data Loader for Fast Sampling-based GNN Training on Large Graphs.*Bai et al.* [[Paper]](https://gnnsys.github.io/papers/GNNSys21_paper_8.pdf) [[Poster]](https://gnnsys.github.io/posters/GNNSys21_poster_8.pdf)
* [**GNNSys Workshop**] Graphiler: A Compiler for Graph Neural Networks.*Xie et al.* [[Paper]](https://gnnsys.github.io/papers/GNNSys21_paper_10.pdf) [[Poster]](https://gnnsys.github.io/posters/GNNSys21_poster_10.pdf)
* [**GNNSys Workshop**] Analyzing the Performance of Graph Neural Networks with Pipe Parallelism.*T. Dearing et al.* [[Paper]](https://gnnsys.github.io/papers/GNNSys21_paper_12.pdf) [[Poster]](https://gnnsys.github.io/posters/GNNSys21_poster_12.pdf)
* [**GNNSys Workshop**] Effiicent Distribution for Deep Learning on Large Graphs.*Hoang et al.* [[Paper]](https://gnnsys.github.io/papers/GNNSys21_paper_16.pdf) [[Poster]](https://gnnsys.github.io/posters/GNNSys21_poster_16.pdf)
* [**GNNSys Workshop**] Load Balancing for Parallel GNN Training.*Su et al.* [[Paper]](https://gnnsys.github.io/papers/GNNSys21_paper_18.pdf) [[Poster]](https://gnnsys.github.io/posters/GNNSys21_poster_18.pdf)

### 2020
* [**ICLR**] GraphSAINT: Graph Sampling Based Inductive Learning Method. *Xue et al.* [[Paper]](https://arxiv.org/pdf/1907.04931)
* [**CVPR**] L2-GCN: Layer-Wise and Learned Efficient Training of Graph Convolutional Networks.*You et al.* [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/You_L2-GCN_Layer-Wise_and_Learned_Efficient_Training_of_Graph_Convolutional_Networks_CVPR_2020_paper.html) 
* [**AccML**] GIN : High-Performance, Scalable Inference for Graph Neural Networks. *Fu et al.* [[Paper]](https://workshops.inf.ed.ac.uk/accml/papers/2020/AccML_2020_paper_6.pdf)
* [**ASAP**] Hardware Acceleration of Large Scale GCN Inference. *Zhang et al.*[[Paper]](https://ieeexplore.ieee.org/document/9153263)
* [**DAC**] Hardware Acceleration of Graph Neural Networks. *Auten, et al.* [[Paper]](https://doi.org/10.1109/dac18072.2020.9218751)
* [**FPGA**] GraphACT: Accelerating GCN Training on CPU-FPGA Heterogeneous Platforms. *Zeng et al.* [[Paper]](https://arxiv.org/abs/2001.02498)
* [**HPCA**] HyGCN: A GCN Accelerator with Hybrid Architecture. *Yan, et al.* [[Paper]](https://doi.org/10.1109/HPCA47549.2020.00012)
* [**IA3**] DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs. *Zheng et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9407264)
* [**ICA3PP**] Towards a Deep-Pipelined Architecture for Accelerating Deep GCN on a Multi-FPGA Platform. *Cheng et al.* [[Paper]](https://link.springer.com/chapter/10.1007/978-3-030-60245-1_36)
* [**ICCAD**] DeepBurning-GL: an automated framework for generating graph neural network accelerators. *Liang et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9256539)
* [**ICCAD**] fuseGNN: accelerating graph convolutional neural network training on GPGPU. *Chen et al.* [[Paper]](xhttps://ieeexplore.ieee.org/abstract/document/9256702/)
* [**ICPADS**] S-GAT: Accelerating Graph Attention Networks Inference on FPGA Platform with Shift Operation. *Yan et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9359183)
* [**ICTAI**] SGQuant: Squeezing the Last Bit on Graph Neural Networks with Specialized Quantization. *Feng et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9288186)
* [**IPDPS**] Pcgcn: Partition-centric processing for accelerating graph convolutional network. *Tian et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9139807/)
* [**ISCA**] GraphABCD: Scaling Out Graph Analytics with Asynchronous Block Coordinate Descent. *Yang et al.* [[Paper]](https://dl.acm.org/doi/10.1109/ISCA45697.2020.00043)
* [**KDD**] Deep Graph Learning: Foundations, Advances and Applications. *Rong et al.* [[Paper]](https://dl.acm.org/doi/10.1145/3394486.3406474)
* [**MLSys**] Improving the Accuracy, Scalability, and Performance of  Graph Neural Networks with Roc. *Jia* [[Paper]](https://www-cs.stanford.edu/people/matei/papers/2020/mlsys_roc.pdf)
* [**SC**] FeatGraph: A Flexible and Efficient Backend for Graph Neural Network Systems. *Hu, et al.* [[Paper]](https://doi.org/10.1109/sc41405.2020.00075.)
* [**SC**] GE-SpMM: General-Purpose Sparse Matrix-Matrix Multiplication on GPUs for Graph Neural Networks. Huang, et al. [[Paper]](https://doi.org/10.1109/sc41405.2020.00076)
* [**SC**] Reducing Communication in Graph Neural Network Training. *Tripathy, et al.* [[Paper]](https://doi.org/10.1109/sc41405.2020.00074)
* [**SoCC**] PaGraph: Scaling GNN Training on Large Graphs via Computation-Aware Caching. *Lin, et al.* [[Paper]](https://doi.org/10.1145/3419111.3421281)
* [**KDD**] TinyGNN: Learning Efficient Graph Neural Networks. *Yan et al.* [[Paper]](https://dl.acm.org/doi/abs/10.1145/3394486.3403236)

### 2019
* [**ASICON**] An FPGA Implementation of GCN with Sparse Adjacency Matrix. *Ding et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/8983647)
* [**ICLR Workshop**] Fast Graph Representation Learning with PyTorch Geometric. *Fey et al.* [[Paper]](https://arxiv.org/abs/1903.02428) [[GitHub]](https://github.com/rusty1s/pytorch_geometric) [[Documentation]](https://pytorch-geometric.readthedocs.io/en/latest/)
* [**IPDPS**] Accurate, efficient and scalable graph embedding. *Zeng et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/8820993)
* [**KDD**] AliGraph: a comprehensive graph neural network platform. *Yang et al.* [[Paper]](https://dl.acm.org/doi/abs/10.1145/3292500.3340404) [[GitHub]](https://github.com/alibaba/graph-learn)
* [**MLSys**] PyTorch-gGraph: A Large-scale Graph Embedding System. *Lerer et al.* [[Paper]](https://dl.acm.org/doi/abs/10.1145/3292500.3340404) [[GitHub]](https://github.com/facebookresearch/PyTorch-BigGraph)
* [**USENIX ATC**] NeuGraph: Parallel Deep Neural Network Computation on Large Graphs. *Ma, et al.* [[Paper]](www.usenix.org/system/files/atc19-ma_0.pdf)

---
## arXiv

### 2022
* [**arXiv**] Marius++: Large-scale training of graph neural networks on a single machine. *Waleffe R, Mohoney J, Rekatsinas T, et al.* [[Paper]](https://arxiv.org/abs/2202.02365)
* [**arXiv**] Low-latency Mini-batch GNN Inference on CPU-FPGA Heterogeneous Platform. *Zhang et al.* [[Paper]](http://arxiv.org/abs/2206.08536)
* [**arXiv**] SmartSAGE: Training Large-scale Graph Neural Networks using In-Storage Processing Architectures. *Lee et al.* [[Paper]](http://arxiv.org/abs/2205.04711)
* [**arXiv**] Hardware/Software Co-Programmable Framework for Computational SSDs to Accelerate Deep Learning Service on Large-Scale Graphs. *Kwon et al.* [[Paper]](http://arxiv.org/abs/2201.09189)
* [**arXiv**] Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis. *Besta et al.* [[Paper]](https://arxiv.org/abs/2205.09702)
* [**arXiv**] SUGAR: Efficient Subgraph-level Training via Resource-aware Graph Partitioning. *Xue et al.* [[Paper]](https://arxiv.org/pdf/2202.00075.pdf)
* [**arXiv**] Survey on Graph Neural Network Acceleration: An Algorithmic Perspective. *Liu, et al.* [[Paper]]( https://arxiv.org/pdf/2202.04822)
* [**arXiv**] FlowGNN: A Dataflow Architecture for Universal Graph Neural Network Inference via Multi-Queue Streaming. *Sarkar et al.* [[Paper]](https://arxiv.org/abs/2204.13103)
* [**arXiv**] GROW: A Row-Stationary Sparse-Dense GEMM Accelerator for Memory-Efficient Graph Convolutional Neural Networks. *Kang et al.* [[Paper]](https://arxiv.org/abs/2203.00158)
* [**arXiv**] Enabling Flexibility for Sparse Tensor Acceleration via Heterogeneity. *Qin et al.* [[Paper]](https://arxiv.org/abs/2201.08916)
* [**arXiv**] Improved Aggregating and Accelerating Training Methods for Spatial Graph Neural Networks on Fraud Detection. *Zeng et al.* [[Paper]](http://arxiv.org/abs/2202.06580)


### 2021
* [**arXiv**] QGTC: Accelerating Quantized GNN via GPU Tensor Core. *Wang et al.* [[Paper]](http://arxiv.org/abs/2111.09547)
* [**arXiv**] Edge-featured Graph Neural Architecture Search.*Cai et al.* [[Paper]](http://arxiv.org/abs/2109.01356)
* [**arXiv**] TC-GNN: Accelerating Sparse Graph Neural Network Computation Via Dense Tensor Core on GPUs. *Wang, et al.* [[Paper]](http://arxiv.org/abs/2112.02052)
* [**arXiv**] GNNIE: GNN Inference Engine with Load-balancing and Graph-Specific Caching. *Mondal et al.* [[Paper]](http://arxiv.org/abs/2105.10554)
* [**arXiv**] Graph Neural Networks: Methods, Applications, and Opportunities. *Waikhom et al.* [[Paper]](http://arxiv.org/abs/2108.10733)
* [**arXiv**] Understanding the Design Space of Sparse/Dense Multiphase Dataflows for Mapping Graph Neural Networks on Spatial Accelerators. *Garg et al.* [[Paper]](https://arxiv.org/pdf/2103.07977)
* [**arXiv**] LW-GCN: A Lightweight FPGA-based Graph Convolutional Network Accelerator. *Tao, et al.* [[Paper]](https://arxiv.org/abs/2111.03184)
* [**arXiv**] GNNSampler: Bridging the Gap between Sampling Algorithms of GNN and Hardware. *Liu et al.* [[Paper]](https://arxiv.org/abs/2108.11571v1)
* [**arXiv**] Sampling methods for efficient training of graph convolutional networks: A survey. *Liu et al.* [[Paper]](https://arxiv.org/abs/2103.05872)
* [**arXiv**] VersaGNN: a Versatile accelerator for Graph neural networks. *Shi et al.* [[Paper]](https://arxiv.org/abs/2105.01280)
* [**arXiv**] ZIPPER: Exploiting Tile- and Operator-level Parallelism for General and Scalable Graph Neural Network Acceleration. *Zhang et al.* [[Paper]](https://arxiv.org/abs/2107.08709v1)
* [**arXiv**] PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models. *Rozemberczki et al.* [[Paper]](https://arxiv.org/abs/2104.07788) [[GitHub]](https://github.com/benedekrozemberczki/pytorch_geometric_temporal)
* [**arXiv**] A Taxonomy for Classification and Comparison of Dataflows for GNN Accelerators. *Garg et al.* [[Paper]](https://arxiv.org/abs/2103.07977)

### 2020
* [**arXiv**] GRIP: A Graph Neural Network Accelerator Architecture. *Kiningham et al.* [[Paper]](https://arxiv.org/abs/2007.13828)
* [**arXiv**] Learned Low Precision Graph Neural Networks. *Zhao et al.* [[Paper]](https://www.euromlsys.eu/pub/zhao21euromlsys.pdf)
* [**arXiv**] Computing Graph Neural Networks: A Survey from Algorithms to Accelerators. *Abadal et al.* [[Paper]](https://arxiv.org/abs/2010.00130)

### 2018
* [**arXiv**] Relational inductive biases, deep learning, and graph networks. *Battaglia1 et al.* [[Paper]](https://arxiv.org/abs/1806.01261) [[GitHub]](https://github.com/deepmind/graph_nets)





