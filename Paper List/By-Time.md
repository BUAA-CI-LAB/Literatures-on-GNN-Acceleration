# Papers For Deep Graph Learning Acceleration
Deep graph learning acceleration papers in a chronological order.

[Back to home](./../README.md)

---
## Journal

### 2021
* [IEEE Computer Architecture Letters] Hardware Acceleration for GCNs via Bidirectional Fusion. *Li et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9425440)
* [JPDC] Accurate, efficient and scalable training of Graph Neural Networks *Zeng et al.* [[Paper]](https://www.sciencedirect.com/science/article/pii/S0743731520303579)
* [JPDC] High performance GPU primitives for graph-tensor learning operations. *Zhang et al.* [[Paper]](https://www.sciencedirect.com/science/article/abs/pii/S0743731520304007)
* [PMLR] A Unified Lottery Ticket Hypothesis for Graph Neural Networks. *Chen et al.* [[Paper]](http://proceedings.mlr.press/v139/chen21p.html)
* [PVLDB] Accelerating Large Scale Real-Time GNN Inference using Channel Pruning. *Zhou et al.* [[Paper]](https://doi.org/10.14778/3461535.3461547)
* [Science China Information Sciences] Towards Efficient Allocation of Graph Convolutional Networks on Hybrid Computation-In-Memory Architecture. *Chen et al.* [[Paper]](https://link.springer.com/article/10.1007/s11432-020-3248-y)
* [TCAD] Cambricon-G: A Polyvalent Energy-efficient Accelerator for Dynamic Graph Neural Networks. *Song et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9326339)
* [TCAD] Rubik: A Hierarchical Architecture for Efficient Graph Neural Network Training. *Chen et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9428002)
* [TPDS] Efficient Data Loader for Fast Sampling-Based GNN Training on Large Graphs. *Bai et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9376972)

### 2020
* [CCIS] GNN-PIM: A Processing-in-Memory Architecture for Graph Neural Networks. *Wang et al.* [[Paper]](https://www.semanticscholar.org/paper/GNN-PIM%3A-A-Processing-in-Memory-Architecture-for-Wang-Guan/1d03e4bebc9cf3c3fdd9204504d92b20d97d1fdf)
* [IEEE Access] FPGAN: An FPGA Accelerator for Graph Attention Networks With Software and Hardware Co-Optimization. *Yan et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9195849)
* [IEEE Computer Architecture Letters] Characterizing and Understanding GCNs on GPU. *Yan et al.* [[Paper]](https://arxiv.org/abs/2010.00130)
* [TC] EnGN: A High-Throughput and Energy-Efficient Accelerator for Large Graph Neural Networks. *Liang et al.* [[Paper]](https://ieeexplore.ieee.org/document/9161360/)
* [TPDS] EDGES: An Efficient Distributed Graph Embedding System on GPU Clusters. *Yang et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9272876)
* [TKDE 2020] Deep Learning on Graphs: A Survey. *Zhang et al.*[[paper]](https://ieeexplore.ieee.org/abstract/document/9039675)

### 2019
* [CoRR 2019] Deep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs. *Wang* [[Paper]](https://arxiv.org/abs/1909.01315v2) [[GitHub]](https://github.com/dmlc/dgl/) [[Home Page]](https://www.dgl.ai/)

---
## Conference

### 2021
* [MLSys&ICLR Workshop 2021] Adaptive Filters and Aggregator Fusion for Efficient Graph Convolutions.*Tailor et al.* [[Paper]](https://arxiv.org/abs/2104.01481) [[GitHub]](https://github.com/shyam196/egc)
* [CVPR] Bi-GCN: Binary Graph Convolutional Network. *Wang et al.* [[Paper]](https://arxiv.org/abs/2010.07565)
* [CVPR] Binary Graph Neural Networks. *Bahri et al.* [[Paper]](https://arxiv.org/abs/2012.15823)
* [DAC] BlockGNN: Towards Efficient GNN Acceleration Using Block-Circulant Weight Matrices. *Zhou, et al.* [[Paper]](https://arxiv.org/abs/2104.06214)
* [DAC] GNNerator: A Hardware/Software Framework for Accelerating Graph Neural Networks. *Stevens et al.* [[Paper]](https://arxiv.org/abs/2103.10836)
* [DATE] ReGraphX: NoC-Enabled 3D Heterogeneous ReRAM Architecture for Training Graph Neural Networks. *Arka, et al.* [[Paper]](https://doi.org/10.23919/DATE51398.2021.9473949)
* [EuroSys] Tesseract: distributed, general graph pattern mining on evolving graphs. *Bindschaedler, et al.* [[Paper]](https://doi.org/10.1145/3447786.3456253)
* [EuroSys] Accelerating Graph Sampling for Graph Machine Learning Using GPUs. *Jangda, et al.* [[Paper]]( https://doi.org/10.1145/3447786.3456244)
* [EuroSys] DGCL: an efficient communication library for distributed GNN training. *Cai et al.* [[Paper]](https://dl.acm.org/doi/abs/10.1145/3447786.3456233)
* [EuroSys] FlexGraph: A Flexible and Efficient Distributed Framework for GNN Training. *Wang, et al.* [[Paper]](https://doi.org/10.1145/3447786.3456229)
* [EuroSys] Seastar: vertex-centric programming for graph neural networks. *Wu, et al.* [[Paper]](https://doi.org/10.1145/3447786.3456247)
* [FCCM] BoostGCN: A Framework for Optimizing GCN Inference on FPGA. *Zhang et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9444065)
* [HPCA] GCNAX: A Flexible and Energy-Efficient Accelerator for Graph Convolutional Neural Networks.  *Li, et al.*  [[Paper]](https://doi.org/10.1109/HPCA51647.2021.00070)
* [ICLR] Degree-Quant: Quantization-Aware Training for Graph Neural Networks. *Tailor et al.* [[Paper]](https://arxiv.org/abs/2008.05000)
* [ICLR (Open Review)] FGNAS: FPGA-AWARE GRAPH NEURAL ARCHITECTURE SEARCH. *Qing et al.* [[Paper]](https://openreview.net/pdf?id=cq4FHzAz9eA) 
* [IPDPS] FusedMM: A Unified SDDMM-SpMM Kernel for Graph Embedding and Graph Neural Networks. *Rahman et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9460486)
* [ISPASS] GNNMark: A Benchmark Suite to Characterize Graph Neural Network Training on GPUs. *Baruah et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9408205)
* [ISPASS] Performance Analysis of Graph Neural Network Frameworks. *Wu et al.* [[Paper]](https://www.semanticscholar.org/paper/Performance-Analysis-of-Graph-Neural-Network-Wu-Sun/b6da3ab0a6e710f16e11e5890818a107d1d5735c)
* [MICRO] AWB-GCN: A Graph Convolutional Network Accelerator with Runtime Workload Rebalancing. *Geng et al.* [[Paper]](https://ieeexplore.ieee.org/document/9252000)
* [OSDI] Dorylus: Affordable, Scalable, and Accurate GNN Training with Distributed CPU Servers and Serverless Threads. *Thorpe et al.* [[Paper]](https://arxiv.org/abs/2105.11118) [[GitHub]](https://github.com/uclasystem/dorylus)
* [PPoPP] Understanding and Bridging the Gaps in Current GNN Performance Optimizations. *Huang, et al.* [[Paper]](https://doi.org/10.1145/3437801.3441585)
* [RTAS] Brief Industry Paper: optimizing Memory Efficiency of Graph Neural Networks on Edge Computing Platforms. *Zhou et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/820993)
* [USENIX ATC] GLIST: Towards In-Storage Graph Learning. *Li, et al.* [[Paper]](www.usenix.org/conference/atc21/presentation/li-cangyuan)
* [GLSVLSI 2021] Co-Exploration of Graph Neural Network and Network-on-Chip Design Using AutoML. *Manu et al.* [[Paper]](https://dl.acm.org/doi/abs/10.1145/3453688.3461741)
* [ICCAD 2021] G-CoS: GNN-Accelerator Co-Search Towards Both Better Accuracy and Efficiency. *Zhang et al.* [[Paper]](https://arxiv.org/pdf/2109.08983.pdf)
* [IJCAI 2021] Automated Machine Learning on Graphs: A Survey. *Zhang et al.* [[Paper]](https://arxiv.org/abs/2103.00742)

### 2020
* [AccML] GIN : High-Performance, Scalable Inference for Graph Neural Networks. *Fu et al.* [[Paper]](https://workshops.inf.ed.ac.uk/accml/papers/2020/AccML_2020_paper_6.pdf)
* [ASAP] Hardware Acceleration of Large Scale GCN Inference. *Zhang et al.*[[Paper]](https://ieeexplore.ieee.org/document/9153263)
* [DAC] Hardware Acceleration of Graph Neural Networks. *Auten, et al.* [[Paper]](https://doi.org/10.1109/dac18072.2020.9218751)
* [FPGA] GraphACT: Accelerating GCN Training on CPU-FPGA Heterogeneous Platforms. *Zeng et al.* [[Paper]](https://arxiv.org/abs/2001.02498)
* [HPCA] HyGCN: A GCN Accelerator with Hybrid Architecture. *Yan, et al.* [[Paper]](https://doi.org/10.1109/HPCA47549.2020.00012)
* [IA3] DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs. *Zheng et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9407264)
* [ICA3PP] Towards a Deep-Pipelined Architecture for Accelerating Deep GCN on a Multi-FPGA Platform. *Cheng et al.* [[Paper]](https://link.springer.com/chapter/10.1007/978-3-030-60245-1_36)
* [ICCAD] DeepBurning-GL: an automated framework for generating graph neural network accelerators. *Liang et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9256539)
* [ICCAD] fuseGNN: accelerating graph convolutional neural network training on GPGPU. *Chen et al.* [[Paper]](xhttps://ieeexplore.ieee.org/abstract/document/9256702/)
* [ICPADS] S-GAT: Accelerating Graph Attention Networks Inference on FPGA Platform with Shift Operation. *Yan et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9359183)
* [ICTAI] SGQuant: Squeezing the Last Bit on Graph Neural Networks with Specialized Quantization. *Feng et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9288186)
* [IPDPS] Pcgcn: Partition-centric processing for accelerating graph convolutional network. *Tian et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/9139807/)
* [ISCA] GraphABCD: Scaling Out Graph Analytics with Asynchronous Block Coordinate Descent. *Yang et al.* [[Paper]](https://dl.acm.org/doi/10.1109/ISCA45697.2020.00043)
(https://ieeexplore.ieee.org/abstract/document/9326339)
* [KDD] Deep Graph Learning: Foundations, Advances and Applications. *Rong et al.* [[Paper]](https://dl.acm.org/doi/10.1145/3394486.3406474)
* [MLSys] Improving the Accuracy, Scalability, and Performance of  Graph Neural Networks with Roc. *Jia* [[Paper]](https://www-cs.stanford.edu/people/matei/papers/2020/mlsys_roc.pdf)
* [SC] FeatGraph: A Flexible and Efficient Backend for Graph Neural Network Systems. *Hu, et al.* [[Paper]](https://doi.org/10.1109/sc41405.2020.00075.)
* [SC] GE-SpMM: General-Purpose Sparse Matrix-Matrix Multiplication on GPUs for Graph Neural Networks. Huang, et al. [[Paper]](https://doi.org/10.1109/sc41405.2020.00076)
* [SC] Reducing Communication in Graph Neural Network Training. *Tripathy, et al.* [[Paper]](https://doi.org/10.1109/sc41405.2020.00074)
* [SoCC] PaGraph: Scaling GNN Training on Large Graphs via Computation-Aware Caching. *Lin, et al.* [[Paper]](https://doi.org/10.1145/3419111.3421281)
* [KDD 2020] TinyGNN: Learning Efficient Graph Neural Networks. *Yan et al.* [[Paper]](https://dl.acm.org/doi/abs/10.1145/3394486.3403236)

### 2019
* [ASICON] An FPGA Implementation of GCN with Sparse Adjacency Matrix. *Ding et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/8983647)
* [ICLR Workshop] Fast Graph Representation Learning with PyTorch Geometric. *Fey et al.* [[Paper]](https://arxiv.org/abs/1903.02428) [[GitHub]](https://github.com/rusty1s/pytorch_geometric) [[Documentation]](https://pytorch-geometric.readthedocs.io/en/latest/)
* [IPDPS] Accurate, efficient and scalable graph embedding. *Zeng et al.* [[Paper]](https://ieeexplore.ieee.org/abstract/document/8820993)
* [KDD] AliGraph: a comprehensive graph neural network platform. *Yang et al.* [[Paper]](https://dl.acm.org/doi/abs/10.1145/3292500.3340404) [[GitHub]](https://github.com/alibaba/graph-learn)
* [MLSys 2019] PyTorch-gGraph: A Large-scale Graph Embedding System. *Lerer et al.* [[Paper]](https://dl.acm.org/doi/abs/10.1145/3292500.3340404) [[GitHub]](https://github.com/facebookresearch/PyTorch-BigGraph)
* [USENIX ATC] NeuGraph: Parallel Deep Neural Network Computation on Large Graphs. *Ma, et al.* [[Paper]](www.usenix.org/system/files/atc19-ma_0.pdf)

---
## arXiv

### 2021
* [arXiv] GNNSampler: Bridging the Gap between Sampling Algorithms of GNN and Hardware. *Liu et al.* [[Paper]](https://arxiv.org/abs/2108.11571v1)
* [arXiv] Sampling methods for efficient training of graph convolutional networks: A survey. *Liu et al.* [[Paper]](https://arxiv.org/abs/2103.05872)
* [arXiv] VersaGNN: a Versatile accelerator for Graph neural networks. *Shi et al.* [[Paper]](https://arxiv.org/abs/2105.01280)
* [arXiv] ZIPPER: Exploiting Tile- and Operator-level Parallelism for General and Scalable Graph Neural Network Acceleration. *Zhang et al.* [[Paper]](https://arxiv.org/abs/2107.08709v1)
* [arXiv] PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural Machine Learning Models. *Rozemberczki et al.* [[Paper]](https://arxiv.org/abs/2104.07788) [[GitHub]](https://github.com/benedekrozemberczki/pytorch_geometric_temporal)
* [arXiv] A Taxonomy for Classification and Comparison of Dataflows for GNN Accelerators. *Garg et al.* [[Paper]](https://arxiv.org/abs/2103.07977)

### 2020
* [arXiv] GRIP: A Graph Neural Network Accelerator Architecture. *Kiningham et al.* [[Paper]](https://arxiv.org/abs/2007.13828)
* [arXiv] Learned Low Precision Graph Neural Networks. *Zhao et al.* [[Paper]](https://www.euromlsys.eu/pub/zhao21euromlsys.pdf)
* [arXiv] Computing Graph Neural Networks: A Survey from Algorithms to Accelerators. *Abadal et al.* [[Paper]](https://arxiv.org/abs/2010.00130)

### 2018
* [arXiv] Relational inductive biases, deep learning, and graph networks. *Battaglia1 et al.* [[Paper]](https://arxiv.org/abs/1806.01261) [[GitHub]](https://github.com/deepmind/graph_nets)






